{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a8839-7c58-4862-a5f0-0f7e97085918",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK TRAINS A FASHION-MNIST MODEL USING 0-255 VALUES (UINT8)\n",
    "\n",
    "> Note that the end model, oexported in a FINN-ONNX format will be Quantized. Quantization is handled by Brevitas.\n",
    "\n",
    "After creating the docker environement,\n",
    "\n",
    "The goal is to create a MNIST Fasion model in pytorch and experiment with the different parameters\n",
    "\n",
    "Then, we will do the same model but fully quantized and start adapting it for FINN\n",
    "\n",
    "> Side note : We'll use Quantization Aware Training (QAT) for this tutorial, but another possibility is to use Post-Training Quantization (PTQ) [Here is a resource to learn more but it's not really necessary for this tutorial](https://www.youtube.com/watch?v=0VdNflU08yA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09c1a7-e82e-4d14-bced-29aaf340638c",
   "metadata": {},
   "source": [
    "## Base model creation\n",
    "\n",
    "Before going anyfurther, note that this notebook was meant to be run in the FINN docker environment, from a linux host.\n",
    "\n",
    "Use the command ```bash run-docker.sh notbook``` and use the jupyter link for your vscode/vim editor or simply open it in your browser if you like jupyter you will also need to setup you env variables right.\n",
    "\n",
    "I encourage you to check the FINN docs for this part, you can find more precise guidelines in the resources of the main [readme](./README.md).\n",
    "\n",
    "> Don't forget to use your own username below. ```root_dir = f\"/tmp/finn_dev_{user_name}\"``` is the common mounted folder where you'll be able to access all outputs from the FINN docker container from you host machine to then use it for further treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e9dec1-299c-45ec-bc1a-41d4825fea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/finn_dev_emre\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "user_name = \"emre\" # REPLACE THIS WITH YOUR HOST MACHINE USER NAME \n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5934e06",
   "metadata": {},
   "source": [
    "Import the data and transform it, we don't normalize to really make it as simple as possible, the only transformation is to convert it to a pytorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9651dd1-25d9-4c48-8e02-08c28d29fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and std\n",
    "]);\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ba53e",
   "metadata": {},
   "source": [
    "Let's visualize the data a little bit... We see the data is ranging from 0 to 1 but we know our FPGA AI model IP will expect INTEGER values... More on that later !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb48f47-1369-4f87-8c9b-c0e29a8de5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min :  0.0  /// Max :  0.78039217\n",
      "Data type : float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[5]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image[0]), \" /// Max : \", np.max(image[0]))\n",
    "print(\"Data type :\", image[0].dtype)\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648a989-431b-418d-9dbf-118ca52aa7e1",
   "metadata": {},
   "source": [
    "# Actual QNN\n",
    "\n",
    "> QNN = Quantized Neural Network\n",
    "\n",
    "That's what we are about to do using Quantized Aware Training, and the best part is Brevitas handles all of it in the background !\n",
    "\n",
    "This part is about creating a quantized version of the model and adapting it to finn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab5b0a0-9cfd-41c7-99cc-3c2d3365cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.nn import QuantReLU\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "brevitas_input_size = 28 * 28\n",
    "brevitas_hidden1 = 64\n",
    "brevitas_hidden2 = 64\n",
    "brevitas_num_classes = 10\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "dropout_prob = 0.5\n",
    "\n",
    "#is this model fully quantized or only the wieghts, i shall dig to find out once done !\n",
    "brevitas_model = nn.Sequential(\n",
    "    QuantLinear(brevitas_input_size, brevitas_hidden1, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden1),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden1, brevitas_hidden2, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden2),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden2, brevitas_num_classes, bias=True, weight_bit_width=weight_bit_width),\n",
    "    QuantReLU(bit_width=act_bit_width)\n",
    ")\n",
    "\n",
    "# uncomment to check the network object\n",
    "# brevitas_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eec74",
   "metadata": {},
   "source": [
    "### The input data has to be quantized.\n",
    "\n",
    "Normaly in brevistas, we can use the ```QuantIdentity()``` layer for this but unfortunatly, it does not convert to hardware (yet) in FINN.\n",
    "\n",
    "It its really not a problem, as we can just quantize the input data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "670acb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom quantization function\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    \n",
    "    return q_x\n",
    "\n",
    "# Define the quantized transform\n",
    "transform_quantized = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x))  # Apply quantization\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform_quantized  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform_quantized\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset_qnt, 100)\n",
    "test_loader = DataLoader(test_dataset_qnt, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84a0bd",
   "metadata": {},
   "source": [
    "Let's re-visualize the data now... That's better !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c819d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min :  0.0  /// Max :  255.0\n",
      "float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset_qnt[10]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image), \" /// Max : \", np.max(image))\n",
    "print(image.dtype)\n",
    "\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a58259",
   "metadata": {},
   "source": [
    "## Actual training\n",
    "\n",
    "Now, just like in a regular PyTorch workflow, we train the model using a simple trainng loop.\n",
    "\n",
    "Note that it takes a bit of time because\n",
    "- We train on CPU\n",
    "- QAT \"simulates\" Quantization using quant/dequant layers (so the model becomes robust to quantization, which is not the case in PTQ) and backpropagation is different and that takes up computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "236d85fe-986a-44f4-84d1-fce1d5591f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7165\n",
      "Epoch [2/5], Loss: 0.6275\n",
      "Epoch [3/5], Loss: 0.6060\n",
      "Epoch [4/5], Loss: 0.5052\n",
      "Epoch [5/5], Loss: 0.4777\n"
     ]
    }
   ],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(brevitas_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "num_epochs = 5\n",
    "brevitas_model.train()\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float()) # This just make the value a float ie 255 becomes 255,0 and not 1\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7ee17",
   "metadata": {},
   "source": [
    "We now test the model with ~85% accuracy, which is extremely close to non-quatized model performances (~85-86% when I tested it on my side when I made the course).\n",
    "\n",
    "This dataset isn't very complex though, this delta will depend on the data you have to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4d239cf-edb2-4faa-a502-d05861a156fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 84.49 %\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "brevitas_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c0b43",
   "metadata": {},
   "source": [
    "Here are some lines to execute the tensor that mmakes up the model and their different representations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3843919-1e83-4c50-accd-3e9292cecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantTensor(value=tensor([[ 0.0296,  0.0000, -0.0296,  ...,  0.0000,  0.0296, -0.0888],\n",
      "        [ 0.0296,  0.0592,  0.0000,  ...,  0.0888,  0.0000,  0.0296],\n",
      "        [-0.0000,  0.0296, -0.0296,  ...,  0.0296,  0.0888,  0.0592],\n",
      "        ...,\n",
      "        [ 0.0000, -0.0592, -0.0592,  ...,  0.0888,  0.0888, -0.0000],\n",
      "        [ 0.0296,  0.0000,  0.0888,  ...,  0.0592,  0.1184,  0.1480],\n",
      "        [-0.0888, -0.0000, -0.0296,  ...,  0.0592,  0.0296, -0.0296]],\n",
      "       grad_fn=<MulBackward0>), scale=tensor(0.0296, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(4.), signed_t=tensor(True), training_t=tensor(False))\n",
      "tensor([[ 1,  0, -1,  ...,  0,  1, -3],\n",
      "        [ 1,  2,  0,  ...,  3,  0,  1],\n",
      "        [ 0,  1, -1,  ...,  1,  3,  2],\n",
      "        ...,\n",
      "        [ 0, -2, -2,  ...,  3,  3,  0],\n",
      "        [ 1,  0,  3,  ...,  2,  4,  5],\n",
      "        [-3,  0, -1,  ...,  2,  1, -1]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "#lets have a quick look at the weights too\n",
    "print(brevitas_model[0].quant_weight())\n",
    "#internally, weoght are stored as float 32, here nare ways to visualize actual quantized weights :\n",
    "print(brevitas_model[0].quant_weight().int())\n",
    "print(brevitas_model[0].quant_weight().int().dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d9899-000b-4343-ac81-2c425b571519",
   "metadata": {},
   "source": [
    "# EXPORTING THE MODEL TO FINN-ONNX\n",
    "\n",
    "maybe you know ONNX, maybe you don't. At the end of the day, It's just a way to represent AI models (or just tensor operations) in an optimised an standard way. It also allows us to use Netron to visualize the model as a nice graph.\n",
    "\n",
    "> FINN-ONNX is just like ONNX, exept it's better for quantized models (under 8bit quantization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f27a3b-88c6-483e-be82-85d1ee53129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n",
      "Model saved to /tmp/finn_dev_emre/ready_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emre/Documents/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "filename = root_dir + \"/part1.onnx\"\n",
    "filename_clean = root_dir + \"/part1_clean.onnx\"\n",
    "\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min = 0\n",
    "    max = 2**num_bits - 1\n",
    "    \n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max\n",
    "    zero_point = np.clip((-beta/scale),0,max).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(np.round(arr / scale + zero_point), min, max).astype(np.float32)\n",
    "    \n",
    "    return quantized_arr\n",
    "\n",
    "#Crete a tensor ressembling the input tensor we saw earlier\n",
    "input_a = np.random.rand(1,28*28).astype(np.float32)\n",
    "input_a = asymmetric_quantize(input_a)\n",
    "print(np.max(input_a[0]))\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    brevitas_model, export_path=filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(filename, out_file=filename_clean)\n",
    "\n",
    "# ModelWrapper\n",
    "model = ModelWrapper(filename_clean)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(root_dir + \"/ready_finn.onnx\")\n",
    "\n",
    "print(\"Model saved to \" + root_dir + \"/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7e045",
   "metadata": {},
   "source": [
    "## Visualization in Netron\n",
    "\n",
    "We can now visualise our network in netron, we can clearly identify each layer in this graph, Great !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d6f0250-5b1c-4276-9293-c29a1b112782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/finn_dev_emre/ready_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0810a9f220>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(root_dir + \"/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db41a1",
   "metadata": {},
   "source": [
    "We are now ready to move on to FINN !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5e72a",
   "metadata": {},
   "source": [
    "# From trained model to HW using the finn compiler\n",
    "\n",
    "We will go through network preparation (mainly through [Strealining](https://arxiv.org/pdf/1709.04060) graph transformations) and conversion to HW using the finn compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0748996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/finn_dev_emre\n"
     ]
    }
   ],
   "source": [
    "user_name = \"emre\" # REPLACE THIS WITH YOUR HOST MACHINE USER NAME \n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "print(root_dir)\n",
    "# get onnx model from the last NOTEBOOK\n",
    "filename = root_dir + \"/ready_finn.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd561449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/ready_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0810a9ee90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "showInNetron(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c41f2e",
   "metadata": {},
   "source": [
    "# Network preparing for finn\n",
    "\n",
    "## Steps summary\n",
    "\n",
    "- Tidy up (and also after EACH step)\n",
    "- Pre (data feed) / Post proc (top k)\n",
    "- **Model streamlining (Main step)** + smaller example\n",
    "- Model HW Layers (Generates Matrix Vector Activation Units for fc layers)\n",
    "- Model data flow partitions (Generate a sub-graph for all HW convertible nodes)\n",
    "- Specialize layer, ready for hw conversion (generates hls for the dataflow partition node)\n",
    "\n",
    "We then adapt the compiler setting depending on the ressources requirements and call FINN API to generate a stitched IP.\n",
    "\n",
    "## Tidy up\n",
    "\n",
    "The goal of these transformation are simply to give node names & identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004aaf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class InferShapes(Transformation):\n",
      "    \"\"\"Ensure every tensor in the model has a specified shape (ValueInfo).\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        # hide your riches!\n",
      "        hidden_ops = _hide_finn_ops(model)\n",
      "        # call regular ONNX shape inference\n",
      "        model = ModelWrapper(si.infer_shapes(model.model))\n",
      "        # bring back hidden ops\n",
      "        _restore_finn_ops(model, hidden_ops)\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model = ModelWrapper(filename)\n",
    "\n",
    "# TIDY UP\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save(root_dir + \"/tidy.onnx\")\n",
    "\n",
    "showSrc(InferShapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b93d8d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0810a9d750>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(root_dir + \"/tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3f110",
   "metadata": {},
   "source": [
    "## Pre processing\n",
    "\n",
    "FINN model will only use 0 to 255 INT8 input once in hgardware, [or at least it will assume integer input](https://github.com/Xilinx/finn/issues/1174#issuecomment-2331778330)\n",
    "\n",
    "We will feed UINT8 quantized values into the model, ranging from 0 to 255, because we did so in training. If the model was trained on FP32 ranging from 0 to 1, adding preprocessing into the graph, like in the commented example below, would be the right solution.\n",
    "\n",
    "We also grab the opportunity to infer the input data type for the FINN compiler to adapt the input interface processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "235552fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/full_preproc.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0808ecd4e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "import torch\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "# PRE PROC : NONE\n",
    "model = ModelWrapper(root_dir + \"/tidy.onnx\")\n",
    "\n",
    "#=========================================================\n",
    "# Here you could add some proprocessing if you need some\n",
    "#=========================================================\n",
    "\n",
    "# add input annotation: UINT8 is what we will feed the model during inference\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "\n",
    "model.save(root_dir + \"/full_preproc.onnx\")\n",
    "showInNetron(root_dir + \"/full_preproc.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf7001",
   "metadata": {},
   "source": [
    "## OPTIONAL : VERIFICATION : TEST THE \"NEW\" MODEL and VERIFY IT\n",
    "\n",
    "Now, we can test this new model in order to check wether we did a good job so far with the different datatypes.\n",
    "\n",
    "To test our new model, we will use the ONNX runtime that allows us to directly run an ONNX model a gather output.\n",
    "\n",
    "> Note that the FINN-ONNX runtime is not very performant but its okay for our purpose\n",
    "\n",
    "Normally, this ONNX runtime is used **EXTENSILVLY** to **VERIFY** everything we do **EVERYTIME WE CHANGE THE MODEL**. In this tutorial, i'll just give you this example so you kno how to use it but given the simplicity of the model, I will only give you this example.\n",
    "\n",
    "### Some examples for confimed users :\n",
    "\n",
    "> Finish this notebook first to fully undertand the following statements :\n",
    "\n",
    "You can see more uses cases of the ONNX Runtime used for verification [here](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/tfc_end2end_verification.ipynb). In this notebook from FINN, they use the ONNX runtime to verfy C++ and RTL functionality of the code generated in the later portions WITHOUT EVER DESIGNING A TESBENCH !!! this is very powerful !\n",
    "\n",
    "[Here](https://github.com/0BAB1/python_to_fpga_course/blob/main/Lab%202%20%3A%20FINN%20Compiler/LAB2_VERIFICATION.ipynb) is a link to the verification notebook from my full 9h course, the notebook may not be as complete but the code is functional on this model. A good exercice for you would be to try and verify these models if you want to ! anyway, it is not andatory for hobby projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e7db130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning the testing loop with ONNX model inference !\n",
      "\n",
      "0 done ! Accuracy so far :  100.0\n",
      "10 done ! Accuracy so far :  100.0\n",
      "20 done ! Accuracy so far :  90.47619047619048\n",
      "30 done ! Accuracy so far :  83.87096774193549\n",
      "40 done ! Accuracy so far :  85.36585365853658\n",
      "50 done ! Accuracy so far :  82.3529411764706\n",
      "60 done ! Accuracy so far :  83.60655737704919\n",
      "70 done ! Accuracy so far :  81.69014084507042\n",
      "80 done ! Accuracy so far :  83.95061728395062\n",
      "90 done ! Accuracy so far :  84.61538461538461\n",
      "accuracy = 86.0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import qonnx.core.onnx_exec as oxe\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/full_preproc.onnx\")\n",
    "\n",
    "#=========================================================\n",
    "# FIRST WE GATHER AND QUANTIZE SOME DATA TO TEST\n",
    "#=========================================================\n",
    "\n",
    "# get some INT 8 data by quantizing some FashionMNIST data\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    \n",
    "    return q_x\n",
    "\n",
    "# Define the quantized transform\n",
    "transform_quantized = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x))  # Apply quantization\n",
    "])\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform_quantized\n",
    ");\n",
    "\n",
    "test_loader_qnt = DataLoader(test_dataset_qnt, 100)\n",
    "\n",
    "#=========================================================\n",
    "# RUN THE TESTING LOOP\n",
    "#=========================================================\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "print(\"\\\\nRunning the testing loop with ONNX model inference !\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader_qnt):\n",
    "        images = torch.reshape(images, (100, 28*28))\n",
    "        for image_idx, image in enumerate(images):\n",
    "            input_tensor = image.detach().numpy().reshape(1, 28 * 28)\n",
    "            # print(input_tensor)\n",
    "            input_dict = {\"global_in\": input_tensor}\n",
    "            output_dict = oxe.execute_onnx(model, input_dict)\n",
    "            produced_qonnx = output_dict[list(output_dict.keys())[0]]\n",
    "            _, predicted = torch.max(torch.from_numpy(produced_qonnx).data, 1)\n",
    "            total += 1\n",
    "            correct += predicted.item() == labels[image_idx].item()\n",
    "            if image_idx % 10 == 0 :\n",
    "                print(image_idx, \"done ! Accuracy so far : \", 100 * correct / total)\n",
    "        if batch_idx == 0 : break # just do 1 batch, too long !\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b281f",
   "metadata": {},
   "source": [
    "## POST PROCESSING\n",
    "\n",
    "We also apply post processing ! In our case we add a \"tok K\" label selector that will directly output the class value with the most credence. We grab the opportunity to do a tidy up again, as we added nodes in the graph that were not there in the fity tidy up graph transformations.\n",
    "\n",
    "After that, we visualize in Netron (as usual, you know the drill now). And we can now see the label selector (topK) at the end of the model and the output shape is now 1x1 aka, a simple scalar designating the class ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5468a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f080e0f6ec0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# POST PROC\n",
    "\n",
    "# insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = root_dir + \"/pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "showInNetron(chkpt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc581507",
   "metadata": {},
   "source": [
    "## Model [streamlining](https://arxiv.org/pdf/1709.04060)\n",
    "\n",
    "This set of transformations are one of the main steps, it infer thesholds in the graph and moves operations around. The end goal is to have a full integers model for inference and a streamlined graph structure easily convertible in HW layers.\n",
    "\n",
    "You can vizualize the set of transformation in [this paper](https://arxiv.org/pdf/1709.04060) or by using ```showSrc(Streamline)```\n",
    "\n",
    "> At the end, you can see in the graph, if you open weights details, that we only have INT remaining, with the exception of a single scalar slace factor at the end, that will later get absorbed in the top K using a special transform in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f36d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f080e0f5330>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "# we can see the list of apllied transformations here : showSrc(Streamline)\n",
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/pre_post.onnx\")\n",
    "# STREAMLINE\n",
    "model = model.transform(Streamline())\n",
    "model.save(root_dir + \"/streamlined.onnx\")\n",
    "showInNetron(root_dir + \"/streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f5356",
   "metadata": {},
   "source": [
    "As we said, there still is this scalar MUL haging around in our network...\n",
    "\n",
    "> Is tha bad ?\n",
    "\n",
    "YES ! it's a floating point scale ! we need to get rid of it !\n",
    "\n",
    "Depending on your architecture, only running the streline transform might not be enough. In that case, you can simply look into the [availible stremline transformations](https://finn.readthedocs.io/en/latest/source_code/finn.transformation.streamline.html) and pick the one that is the most suitable to fully end you model preparation/streamlining process.\n",
    "\n",
    "> Note : depending on your architecture, you might be in a scenario where **NO MODEL TRANSFORMATION IS ENOUGH**. This can happen, you can open an issue in the [FINN GitHub](https://github.com/Xilinx/finn/tree/main/notebooks) and see from there what course of action to take. If really needed, you can operate some operation in sofware using a CPU alongside you FPGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5345c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emre/Documents/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:136: UserWarning: Assuming 2D input is NC\n",
      "  warnings.warn(\"Assuming 2D input is NC\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/finn_dev_emre/streamlined_merged_and_ready.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f080e0f4af0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/streamlined.onnx\")\n",
    "\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(root_dir + \"/streamlined_merged_and_ready.onnx\")\n",
    "showInNetron(root_dir + \"/streamlined_merged_and_ready.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b46d6",
   "metadata": {},
   "source": [
    "# Convert to HW\n",
    "\n",
    "## Generate HW Layers : MVAU\n",
    "\n",
    "This step does not generate HLS nor RTL code, but rather merges thresholds and matrix vector operations (or convolutions if you use conv nets) into Matrix Vector Activation Units (or Sliding Window Units), if possible,  that each represent a layer that FINN can easily work with. Note that an input preprocessing quantiazer will be implemented as a simple standalone Threasholding layer that will then nbe converted to Thresholding_hls layer.\n",
    "\n",
    "See the [FINN PAPER](https://arxiv.org/pdf/1612.07119) Section 4 (page 4 to 6+ especially) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c01c08fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/hw.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f080e0ae2f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# TO HW LAYERS\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/streamlined_merged_and_ready.onnx\")\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "model = model.transform(to_hw.InferChannelwiseLinearLayer())\n",
    "model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model.save(root_dir + \"/hw.onnx\")\n",
    "showInNetron(root_dir + \"/hw.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915df57c",
   "metadata": {},
   "source": [
    "### Isolate HW convertible layers.\n",
    "\n",
    "Once MVAU are infered, we can ask FINN to discriminate HW convertible layers into dataflow partitions.\n",
    "\n",
    "> In this example, everything is convertible to HW, meaning the whole model will be transfered in the \"streaming dataflow partition\" and that the whole thing will get implemented on FPGA (aka no need to do sofware processing using any kind of CPU) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f52e1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/df_part.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f080e0f7910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/hw.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(root_dir + \"/df_part.onnx\")\n",
    "showInNetron(root_dir + \"/df_part.onnx\")\n",
    "\n",
    "# We see that our entire model is now part of a child model in \"StreamingDataflowPartition\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46186faa",
   "metadata": {},
   "source": [
    "We select the child streaming dataflow model to work on it (i.e. do the actual HLS/RTL conversion) using [getCustopOp](https://finn.readthedocs.io/en/latest/source_code/finn.custom_op.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626f1ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/dataflow_partition_ysw42mbv/partition_0.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f080e0f7a90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "showInNetron(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea8c89",
   "metadata": {},
   "source": [
    "## MVAU HLS Conversion\n",
    "\n",
    "In this process, we simply runnn a FINN API call that will generate HLS Layers from the MVAU / SLWU. To adapt the resulting HLS code, we also chose an FPGA part from the supported pool. You can simply choose a part that \"ressembles\" your end target as this is not the end Synth target for Vitis HLS RTL conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6db4db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ultra96', 'Ultra96-V2', 'Pynq-Z1', 'Pynq-Z2', 'ZCU102', 'ZCU104', 'ZCU111', 'RFSoC2x2', 'RFSoC4x2', 'KV260_SOM'])\n"
     ]
    }
   ],
   "source": [
    "# print the names of the supported PYNQ boards\n",
    "from finn.util.basic import pynq_part_map\n",
    "print(pynq_part_map.keys())\n",
    "\n",
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Pynq-Z1\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e885fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class SpecializeLayers(Transformation):\n",
      "    \"\"\"Specialize all layers to either HLS or RTL variants\"\"\"\n",
      "\n",
      "    def __init__(self, fpgapart):\n",
      "        super().__init__()\n",
      "        self.fpgapart = fpgapart\n",
      "\n",
      "    def apply(self, model):\n",
      "        graph = model.graph\n",
      "        node_ind = 0\n",
      "        graph_modified = False\n",
      "        for node in graph.node:\n",
      "            # Skip nodes that are not hw layers\n",
      "            if not node.domain == \"finn.custom_op.fpgadataflow\":\n",
      "                continue\n",
      "            node_ind += 1\n",
      "            impl_style = _determine_impl_style(node, self.fpgapart, model)\n",
      "            optype = node.op_type + \"_\" + impl_style\n",
      "\n",
      "            new_node = helper.make_node(\n",
      "                optype,\n",
      "                node.input,\n",
      "                node.output,\n",
      "                domain=\"finn.custom_op.fpgadataflow.\" + impl_style,\n",
      "            )\n",
      "            # add all attributes\n",
      "            for attribute in node.attribute:\n",
      "                if attribute.name != \"preferred_impl_style\":\n",
      "                    new_node.attribute.append(attribute)\n",
      "            graph.node.insert(node_ind, new_node)\n",
      "            # remove old nodes\n",
      "            graph.node.remove(node)\n",
      "            graph_modified = True\n",
      "        return (model, graph_modified)\n",
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/to_hw_conv.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f08093cd720>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "model = ModelWrapper(dataflow_model_filename)\n",
    "\n",
    "model = model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "showSrc(SpecializeLayers)\n",
    "\n",
    "model.save(root_dir + \"/to_hw_conv.onnx\")\n",
    "showInNetron(root_dir + \"/to_hw_conv.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e4cb6",
   "metadata": {},
   "source": [
    "## OPTIONAL : FOLDING PARAMETER AND LOOP UNROLLING\n",
    "\n",
    "> This part is optional but gives you control over the ressource usage. You can skip it if you want.\n",
    "\n",
    "Folding is another way to describe the **\"Loop unrolling\"** process, aka, how much computing you want to run in parallel.\n",
    "\n",
    "It basically is the part where you do tradeoffs between speed and ressource usage.\n",
    "\n",
    "When dealing with ressource requirements, tinkering these parameters is the ways to go in order to find a balance between throughput and ressource utilization. Here are some examples from the [finn paper, table 3 on page 8](https://arxiv.org/pdf/1612.07119) on BNNs and the effect of maximize and minimizing the usage :\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "### regarding the effects\n",
    "\n",
    "Some vocabulary first :\n",
    "\n",
    "- PE = Processing Elements => aka actual computing units that will run [MAC](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation) operations.\n",
    "   - => Theorically improves computing throughput.\n",
    "- SIMD = [Single Instruction Multiple Data](https://fr.wikipedia.org/wiki/Single_instruction_multiple_data) => aka a way to feed MULTIPLE data at ONCE in a signle PE\n",
    "   - => Theorically improves datapath bandwidth.\n",
    "\n",
    "> In general, the higher the PE and SIMD values are set, the faster the generated accelerator will run, and the more FPGA resources ([LUTs](https://en.wikipedia.org/wiki/Lookup_table), see the \"*Hardware LUTs*\" chapter) it will use. \n",
    "\n",
    "See [finn paper, figure 5 on page 5](https://arxiv.org/pdf/1612.07119) or refer to the lectures for better understanding of PE, SIMD, LOOP UNROLLING & FOLDING\n",
    "\n",
    "### Gather information on current params\n",
    "\n",
    "> Running the following cell of code will you you infos on the current folding parameters\n",
    "\n",
    "Note that the default \"0\" value means that you let FINN sort it out for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f611d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomOp wrapper is of class MVAU_hls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PE': ('i', True, 0),\n",
       " 'SIMD': ('i', True, 0),\n",
       " 'MW': ('i', True, 0),\n",
       " 'MH': ('i', True, 0),\n",
       " 'resType': ('s', False, 'auto', {'auto', 'dsp', 'lut'}),\n",
       " 'ActVal': ('i', False, 0),\n",
       " 'inputDataType': ('s', True, ''),\n",
       " 'weightDataType': ('s', True, ''),\n",
       " 'outputDataType': ('s', True, ''),\n",
       " 'accDataType': ('s', False, 'INT32'),\n",
       " 'binaryXnorMode': ('i', False, 0, {0, 1}),\n",
       " 'noActivation': ('i', False, 0, {0, 1}),\n",
       " 'numInputVectors': ('ints', False, [1]),\n",
       " 'mem_mode': ('s',\n",
       "  False,\n",
       "  'internal_decoupled',\n",
       "  {'external', 'internal_decoupled', 'internal_embedded'}),\n",
       " 'ram_style': ('s', False, 'auto', {'auto', 'block', 'distributed', 'ultra'}),\n",
       " 'ram_style_thresholds': ('s',\n",
       "  False,\n",
       "  'auto',\n",
       "  {'auto', 'block', 'distributed'}),\n",
       " 'runtime_writeable_weights': ('i', False, 0, {0, 1}),\n",
       " 'backend': ('s', True, 'fpgadataflow'),\n",
       " 'preferred_impl_style': ('s', False, '', {'', 'hls', 'rtl'}),\n",
       " 'code_gen_dir_ipgen': ('s', False, ''),\n",
       " 'ipgen_path': ('s', False, ''),\n",
       " 'ip_path': ('s', False, ''),\n",
       " 'ip_vlnv': ('s', False, ''),\n",
       " 'exec_mode': ('s', False, '', {'', 'cppsim', 'rtlsim'}),\n",
       " 'cycles_rtlsim': ('i', False, 0),\n",
       " 'cycles_estimate': ('i', False, 0),\n",
       " 'rtlsim_trace': ('s', False, ''),\n",
       " 'res_estimate': ('s', False, ''),\n",
       " 'res_synth': ('s', False, ''),\n",
       " 'rtlsim_so': ('s', False, ''),\n",
       " 'slr': ('i', False, -1),\n",
       " 'mem_port': ('s', False, ''),\n",
       " 'partition_id': ('i', False, 0),\n",
       " 'device_id': ('i', False, 0),\n",
       " 'inFIFODepths': ('ints', False, [2]),\n",
       " 'outFIFODepths': ('ints', False, [2]),\n",
       " 'output_hook': ('s', False, ''),\n",
       " 'io_chrc_in': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_out': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_period': ('i', False, 0),\n",
       " 'io_chrc_pads_in': ('ints', False, []),\n",
       " 'io_chrc_pads_out': ('ints', False, []),\n",
       " 'code_gen_dir_cppsim': ('s', False, ''),\n",
       " 'executable_path': ('s', False, ''),\n",
       " 'res_hls': ('s', False, '')}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(root_dir + \"/to_hw_conv.onnx\")\n",
    "\n",
    "fc0 = model.graph.node[0] #1st MVAU\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "\n",
    "fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ec2ee",
   "metadata": {},
   "source": [
    "### Set the folding \"automatically\" using SetFolding\n",
    "\n",
    "See manual example in this [finn notebook](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/tfc_end2end_example.ipynb).\n",
    "\n",
    "SetFolding will automatically try to figure out PE ans SIMD parameter depending on target requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50fc1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "\n",
    "TARGET_FPS = 1000000\n",
    "target_clk_cycles_per_frame = 1/(TARGET_FPS * 10e-9)\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/to_hw_conv.onnx\")\n",
    "# Actual method SetFolding that will automatically modify the folding settings\n",
    "model.transform(SetFolding(target_cycles_per_frame=target_clk_cycles_per_frame, mvau_wwidth_max=64, two_pass_relaxation=True))\n",
    "model.save(root_dir + \"/to_hw_folded.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8585b",
   "metadata": {},
   "source": [
    "### For experts :\n",
    "\n",
    "Note that you can also use [this transformation](https://github.com/Xilinx/finn/blob/9d299689f2ec0895f208b8bfe3bcdcf6f450181a/src/finn/transformation/fpgadataflow/set_fifo_depths.py#L199) to modify the input fifos and output fifos depth of each layer if needed. I never had problems with this so I'll let do your own reasearch if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4291582",
   "metadata": {},
   "source": [
    "# ACTUAL HARDWARE BUILD\n",
    "\n",
    "## First, run an estimate\n",
    "\n",
    "We first run some estimations based on the TARGET FPS constant defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "368596a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n",
      "Building dataflow accelerator from /tmp/finn_dev_emre/to_hw_folded.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_emre\n",
      "Final outputs will be generated in output_estimates_only\n",
      "Build log is at output_estimates_only/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/10]\n",
      "Running step: step_tidy_up [2/10]\n",
      "Running step: step_streamline [3/10]\n",
      "Running step: step_convert_to_hw [4/10]\n",
      "Running step: step_create_dataflow_partition [5/10]\n",
      "Running step: step_specialize_layers [6/10]\n",
      "Running step: step_target_fps_parallelization [7/10]\n",
      "Running step: step_apply_folding_config [8/10]\n",
      "Running step: step_minimize_bit_width [9/10]\n",
      "Running step: step_generate_estimate_reports [10/10]\n",
      "Completed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ESTIMATE\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = root_dir + \"/to_hw_folded.onnx\"\n",
    "\n",
    "estimates_output_dir = \"output_estimates_only\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = TARGET_FPS,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")\n",
    "\n",
    "build.build_dataflow_cfg(model_file, cfg_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d2126",
   "metadata": {},
   "source": [
    "### Read the estimates reports\n",
    "\n",
    "In these reports, we can see the estimates fits our target, and we can get an overview of hw many LUTs will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20f8cc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"critical_path_cycles\": 252,\n",
      "  \"max_cycles\": 98,\n",
      "  \"max_cycles_node_name\": \"MVAU_hls_0\",\n",
      "  \"estimated_throughput_fps\": 1020408.1632653062,\n",
      "  \"estimated_latency_ns\": 2520.0\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {estimates_output_dir}/report/estimate_network_performance.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975dbdd8",
   "metadata": {},
   "source": [
    "If you did the folding part, you can check here if your estimated FPS throughput is close to your TARGET_FPS. During my tesing, I had a 1 020 408 FPS estimate for a 1 000 000 Target, which is pretty close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ccd79c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MVAU_hls_0': 98, 'MVAU_hls_1': 64, 'MVAU_hls_2': 80, 'LabelSelect_hls_0': 10}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def read_json_dict(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ret = json.load(f)\n",
    "    return ret\n",
    "\n",
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_cycles.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c5c0e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MVAU_hls_0': {'BRAM_18K': 57,\n",
       "  'BRAM_efficiency': 0.19103313840155944,\n",
       "  'LUT': 34338,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_1': {'BRAM_18K': 8,\n",
       "  'BRAM_efficiency': 0.1111111111111111,\n",
       "  'LUT': 3133,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_2': {'BRAM_18K': 1,\n",
       "  'BRAM_efficiency': 0.1388888888888889,\n",
       "  'LUT': 657,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'LabelSelect_hls_0': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 0,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'total': {'BRAM_18K': 66.0, 'LUT': 38128.0, 'URAM': 0.0, 'DSP': 0.0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_resources.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b7c5e",
   "metadata": {},
   "source": [
    "## Run the actual harware build\n",
    "\n",
    "> This step can take a lot of time.\n",
    "\n",
    "> MAke sure you did your dev enrionement setup right, aka all the ENV varibles that should point on the xilinx tools are sourced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f21059bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emre/Documents/finn/src/finn/transformation/fpgadataflow/floorplan.py:107: UserWarning: 6 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/emre/Documents/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/emre/Documents/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:294: UserWarning: Output FIFO for LabelSelect_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/emre/Documents/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py:290: UserWarning: First node is not StreamingFIFO or IODMA.\n",
      "                You may experience incorrect stitched-IP rtlsim or hardware\n",
      "                behavior. It is strongly recommended to insert FIFOs prior to\n",
      "                calling CreateStitchedIP.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Actual hardware build, ZYNQ BUILD\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import ReplaceVerilogRelPaths\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/to_hw_conv.onnx\")\n",
    "\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns,partition_model_dir=\"./test\",enable_debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8271bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(root_dir + \"/post_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97b9b6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_emre/post_synth.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f08137257e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(root_dir + \"/post_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69ad2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(root_dir + \"/post_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf7f4003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[key: \"floorplan_json\"\n",
       "value: \"/tmp/finn_dev_emre/vitis_floorplan_fuv137o9/floorplan.json\"\n",
       ", key: \"vivado_pynq_proj\"\n",
       "value: \"/tmp/finn_dev_emre/vivado_zynq_proj_8udz2zj5\"\n",
       ", key: \"bitfile\"\n",
       "value: \"/tmp/finn_dev_emre/vivado_zynq_proj_8udz2zj5/resizer.bit\"\n",
       ", key: \"hw_handoff\"\n",
       "value: \"/tmp/finn_dev_emre/vivado_zynq_proj_8udz2zj5/resizer.hwh\"\n",
       ", key: \"vivado_synth_rpt\"\n",
       "value: \"/tmp/finn_dev_emre/vivado_zynq_proj_8udz2zj5/synth_report.xml\"\n",
       ", key: \"platform\"\n",
       "value: \"zynq-iodma\"\n",
       "]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.metadata_props"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b1dfb",
   "metadata": {},
   "source": [
    "## Some output IPs examples :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44f30d",
   "metadata": {},
   "source": [
    "![image.png](attachment:62288775-2dba-4775-8510-34eb8f89ffe6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb946b",
   "metadata": {},
   "source": [
    "![image.png](attachment:2557d696-c367-4597-890c-b340ad8fb3ee.png)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
